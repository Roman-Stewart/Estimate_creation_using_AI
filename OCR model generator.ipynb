{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35e33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac77633",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23656047",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829aa772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e8d01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d402d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d1fc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a528c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fde476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c136155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9b0902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bec009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a286521d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64222ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434beb2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa8467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "624c4767",
   "metadata": {},
   "source": [
    "# 모델 새로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3929cf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9/9 [==============================] - 4s 356ms/step - loss: 5.7174 - accuracy: 0.0576 - val_loss: 6.1429 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 3s 349ms/step - loss: 4.0123 - accuracy: 0.1331 - val_loss: 4.9569 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 3s 344ms/step - loss: 3.2498 - accuracy: 0.2662 - val_loss: 4.6950 - val_accuracy: 0.0143\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 3s 332ms/step - loss: 2.6425 - accuracy: 0.4173 - val_loss: 4.5841 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 3s 333ms/step - loss: 1.9425 - accuracy: 0.5791 - val_loss: 4.0318 - val_accuracy: 0.0286\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "예측된 레이블: 유앙페\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "# JSON 파일 경로\n",
    "json_file_path = 'C:/upload/IMG_OCR_53_4PO_09451.json'\n",
    "\n",
    "# JSON 파일 읽고 지정된 범위의 데이터 추출\n",
    "with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    json_lines = json_file.readlines()[29:-1]\n",
    "\n",
    "# JSON 데이터 파싱 및 \"bbox\" 부분 제거\n",
    "json_data = ''.join(line.strip().replace('\"bbox\": ', '') for line in json_lines)\n",
    "data = json.loads(json_data)\n",
    "\n",
    "dict_list = data\n",
    "\n",
    "# 이미지 로드 및 전처리 함수 정의\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (128, 128))\n",
    "    return image\n",
    "\n",
    "# 이미지 데이터와 레이블 준비\n",
    "image = cv2.imread(\"C:/upload/image1.png\")\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for dict in dict_list:\n",
    "    x1 = min(dict[\"x\"])\n",
    "    x2 = max(dict[\"x\"])\n",
    "    y1 = min(dict[\"y\"])\n",
    "    y2 = max(dict[\"y\"])\n",
    "    box_image = image[y1:y2, x1:x2]\n",
    "    box_image = cv2.cvtColor(box_image, cv2.COLOR_BGR2GRAY)\n",
    "    box_image = cv2.resize(box_image, (128, 128))\n",
    "    X.append(box_image)\n",
    "    y.append(dict[\"data\"])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_onehot = to_categorical(y_encoded, num_classes=len(label_encoder.classes_))\n",
    "\n",
    "\n",
    "# 데이터 확장을 위한 이미지 변환 함수 정의\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def augment_image(image):\n",
    "    # 이미지 변환 방식 무작위 선택\n",
    "    transform_type = random.choice([\"rotate\", \"brightness\", \"color\", \"stretch\", \"noise\"])\n",
    "\n",
    "    if transform_type == \"rotate\":\n",
    "        angle = random.randint(-30, 30)\n",
    "        rows, cols = image.shape\n",
    "        rotation_matrix = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
    "        transformed_image = cv2.warpAffine(image, rotation_matrix, (cols, rows))\n",
    "    elif transform_type == \"brightness\":\n",
    "        brightness_factor = random.uniform(0.7, 1.3)\n",
    "        transformed_image = cv2.convertScaleAbs(image, alpha=brightness_factor, beta=0)\n",
    "    elif transform_type == \"color\":\n",
    "        color_factor = random.uniform(0.5, 1.5)\n",
    "        transformed_image = image * color_factor\n",
    "        transformed_image = np.clip(transformed_image, 0, 255).astype(np.uint8)\n",
    "    elif transform_type == \"stretch\":\n",
    "        stretch_factor_x = random.uniform(0.8, 1.2)\n",
    "        stretch_factor_y = random.uniform(0.8, 1.2)\n",
    "        rows, cols = image.shape\n",
    "        stretched_image = cv2.resize(image, (int(cols * stretch_factor_x), int(rows * stretch_factor_y)))\n",
    "        transformed_image = cv2.resize(stretched_image, (128, 128))\n",
    "    elif transform_type == \"noise\":\n",
    "        gaussian_noise = np.zeros((image.shape[0], image.shape[1], 1), dtype=np.uint8)\n",
    "        cv2.randn(gaussian_noise, 128, 12)\n",
    "        noisy_image = cv2.add(image, gaussian_noise)\n",
    "        transformed_image = np.clip(noisy_image, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "\n",
    "augmented_X = []\n",
    "augmented_y_onehot = []\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    original_image = X[i]\n",
    "    augmented_X.append(original_image)\n",
    "    augmented_y_onehot.append(y_onehot[i])\n",
    "\n",
    "    # 이미지 확장을 위해 변환된 이미지 추가\n",
    "    for _ in range(5):  # 30개의 변환된 이미지 추가\n",
    "        transformed_image = augment_image(original_image)\n",
    "        sized_transformed_image = cv2.resize(transformed_image, (128, 128))\n",
    "        augmented_X.append(sized_transformed_image)\n",
    "        augmented_y_onehot.append(y_onehot[i])\n",
    "\n",
    "# 데이터 확장 후 배열로 변환\n",
    "augmented_X = np.array(augmented_X)\n",
    "augmented_y_onehot = np.array(augmented_y_onehot)\n",
    "\n",
    "# 훈련 및 검증 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(augmented_X, augmented_y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 생성\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(128, 128, 1)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 조기 종료 콜백 정의\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# 새로운 이미지 예측 함수 정의\n",
    "def predict_new_image(image_path, model, label_encoder):\n",
    "    new_image = load_and_preprocess_image(image_path)\n",
    "    prediction = model.predict(np.array([new_image]))\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    return predicted_label\n",
    "\n",
    "# 새로운 이미지 예측\n",
    "new_image_path = 'C:/upload/image3.png'\n",
    "predicted_label = predict_new_image(new_image_path, model, label_encoder)\n",
    "print(\"예측된 레이블:\", predicted_label)\n",
    "\n",
    "\n",
    "# 모델 저장\n",
    "model_filepath = 'C:/upload/handwriting_model.h5'\n",
    "label_encoder_filepath = 'C:/upload/handwriting_label_encoder.pkl'\n",
    "\n",
    "import pickle\n",
    "# 모델 저장 함수 정의\n",
    "def save_model(model, model_filepath, label_encoder_filepath):\n",
    "    model.save(model_filepath)\n",
    "\n",
    "    with open(label_encoder_filepath, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "\n",
    "        \n",
    "save_model(model, model_filepath, label_encoder_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a672c3aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_new_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m new_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/upload/image6.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m predict_new_image(new_image_path, model, label_encoder)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m예측된 레이블:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predicted_label)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict_new_image' is not defined"
     ]
    }
   ],
   "source": [
    "new_image_path = 'C:/upload/image6.png'\n",
    "predicted_label = predict_new_image(new_image_path, model, label_encoder)\n",
    "print(\"예측된 레이블:\", predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c73268",
   "metadata": {},
   "source": [
    "# 기존 모델에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3cea093",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filepath = 'C:/upload/handwriting_model.h5'\n",
    "label_encoder_filepath = 'C:/upload/handwriting_label_encoder.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "604b2772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_filepath, label_encoder_filepath):\n",
    "    loaded_model = keras.models.load_model(model_filepath)\n",
    "\n",
    "    with open(label_encoder_filepath, 'rb') as f:\n",
    "        loaded_label_encoder = pickle.load(f)\n",
    "\n",
    "    return loaded_model, loaded_label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "903ff212",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model, loaded_label_encoder = load_model(model_filepath, label_encoder_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e13c4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9/9 [==============================] - 4s 370ms/step - loss: 2.2283 - accuracy: 0.5755 - val_loss: 3.5844 - val_accuracy: 0.0714\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 3s 336ms/step - loss: 1.8630 - accuracy: 0.6259 - val_loss: 3.2590 - val_accuracy: 0.2286\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 3s 339ms/step - loss: 1.4370 - accuracy: 0.7050 - val_loss: 2.9403 - val_accuracy: 0.3429\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 3s 340ms/step - loss: 1.1616 - accuracy: 0.7626 - val_loss: 2.6648 - val_accuracy: 0.4429\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 3s 332ms/step - loss: 1.0095 - accuracy: 0.8058 - val_loss: 2.3892 - val_accuracy: 0.5429\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "예측된 레이블: 유앙페\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "# JSON 파일 경로\n",
    "json_file_path = 'C:/upload/IMG_OCR_53_4PO_09451.json'\n",
    "\n",
    "# JSON 파일 읽고 지정된 범위의 데이터 추출\n",
    "with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    json_lines = json_file.readlines()[29:-1]\n",
    "\n",
    "# JSON 데이터 파싱 및 \"bbox\" 부분 제거\n",
    "json_data = ''.join(line.strip().replace('\"bbox\": ', '') for line in json_lines)\n",
    "data = json.loads(json_data)\n",
    "\n",
    "dict_list = data\n",
    "\n",
    "# 이미지 로드 및 전처리 함수 정의\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (128, 128))\n",
    "    return image\n",
    "\n",
    "# 이미지 데이터와 레이블 준비\n",
    "image = cv2.imread(\"C:/upload/image1.png\")\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for dict in dict_list:\n",
    "    x1 = min(dict[\"x\"])\n",
    "    x2 = max(dict[\"x\"])\n",
    "    y1 = min(dict[\"y\"])\n",
    "    y2 = max(dict[\"y\"])\n",
    "    box_image = image[y1:y2, x1:x2]\n",
    "    box_image = cv2.cvtColor(box_image, cv2.COLOR_BGR2GRAY)\n",
    "    box_image = cv2.resize(box_image, (128, 128))\n",
    "    X.append(box_image)\n",
    "    y.append(dict[\"data\"])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "label_encoder = loaded_label_encoder\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_onehot = to_categorical(y_encoded, num_classes=len(label_encoder.classes_))\n",
    "\n",
    "\n",
    "# 데이터 확장을 위한 이미지 변환 함수 정의\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def augment_image(image):\n",
    "    # 이미지 변환 방식 무작위 선택\n",
    "    transform_type = random.choice([\"rotate\", \"brightness\", \"color\", \"stretch\", \"noise\"])\n",
    "\n",
    "    if transform_type == \"rotate\":\n",
    "        angle = random.randint(-30, 30)\n",
    "        rows, cols = image.shape\n",
    "        rotation_matrix = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
    "        transformed_image = cv2.warpAffine(image, rotation_matrix, (cols, rows))\n",
    "    elif transform_type == \"brightness\":\n",
    "        brightness_factor = random.uniform(0.7, 1.3)\n",
    "        transformed_image = cv2.convertScaleAbs(image, alpha=brightness_factor, beta=0)\n",
    "    elif transform_type == \"color\":\n",
    "        color_factor = random.uniform(0.5, 1.5)\n",
    "        transformed_image = image * color_factor\n",
    "        transformed_image = np.clip(transformed_image, 0, 255).astype(np.uint8)\n",
    "    elif transform_type == \"stretch\":\n",
    "        stretch_factor_x = random.uniform(0.8, 1.2)\n",
    "        stretch_factor_y = random.uniform(0.8, 1.2)\n",
    "        rows, cols = image.shape\n",
    "        stretched_image = cv2.resize(image, (int(cols * stretch_factor_x), int(rows * stretch_factor_y)))\n",
    "        transformed_image = cv2.resize(stretched_image, (128, 128))\n",
    "    elif transform_type == \"noise\":\n",
    "        gaussian_noise = np.zeros((image.shape[0], image.shape[1], 1), dtype=np.uint8)\n",
    "        cv2.randn(gaussian_noise, 128, 12)\n",
    "        noisy_image = cv2.add(image, gaussian_noise)\n",
    "        transformed_image = np.clip(noisy_image, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "\n",
    "augmented_X = []\n",
    "augmented_y_onehot = []\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    original_image = X[i]\n",
    "    augmented_X.append(original_image)\n",
    "    augmented_y_onehot.append(y_onehot[i])\n",
    "\n",
    "    # 이미지 확장을 위해 변환된 이미지 추가\n",
    "    for _ in range(5):  # 30개의 변환된 이미지 추가\n",
    "        transformed_image = augment_image(original_image)\n",
    "        sized_transformed_image = cv2.resize(transformed_image, (128, 128))\n",
    "        augmented_X.append(sized_transformed_image)\n",
    "        augmented_y_onehot.append(y_onehot[i])\n",
    "\n",
    "# 데이터 확장 후 배열로 변환\n",
    "augmented_X = np.array(augmented_X)\n",
    "augmented_y_onehot = np.array(augmented_y_onehot)\n",
    "\n",
    "# 훈련 및 검증 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(augmented_X, augmented_y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = loaded_model\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# 새로운 이미지 예측 함수 정의\n",
    "def predict_new_image(image_path, model, label_encoder):\n",
    "    new_image = load_and_preprocess_image(image_path)\n",
    "    prediction = model.predict(np.array([new_image]))\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    return predicted_label\n",
    "\n",
    "# 새로운 이미지 예측\n",
    "new_image_path = 'C:/upload/image3.png'\n",
    "predicted_label = predict_new_image(new_image_path, model, label_encoder)\n",
    "print(\"예측된 레이블:\", predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e1792c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cce4d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model_filepath = 'C:/upload/new_handwriting_model.h5'\n",
    "label_encoder_filepath = 'C:/upload/new_handwriting_label_encoder.pkl'\n",
    "\n",
    "import pickle\n",
    "# 모델 저장 함수 정의\n",
    "def save_model(model, model_filepath, label_encoder_filepath):\n",
    "    model.save(model_filepath)\n",
    "\n",
    "    with open(label_encoder_filepath, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "\n",
    "        \n",
    "save_model(model, model_filepath, label_encoder_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1686187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8565907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb30077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d2e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa171d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f977d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b319efd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c7034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bcb712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20284b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c43cf6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d4499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5b715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d76dff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b0ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2112e371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a89863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e46237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74fa32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac75571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c37e9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc352849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f8c896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c8277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcfc16e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96347a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f6fd54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d9049",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c3149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c968ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be582971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4e336f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f932865",
   "metadata": {},
   "source": [
    "# XXXXXXXXXXXXXXXXXXXXXX 이전 버전XXXXXXXXXXXXXXXXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77819f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(r\"C:/upload/handwriting_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2614d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r\"C:/upload/handwriting_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76357151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99a53c72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'data': '09451',\n",
       "  'id': 1,\n",
       "  'x': [1848, 1848, 2019, 2019],\n",
       "  'y': [199, 275, 199, 275]},\n",
       " {'data': '먹방', 'id': 2, 'x': [812, 812, 938, 938], 'y': [653, 726, 653, 726]},\n",
       " {'data': '공맹수',\n",
       "  'id': 3,\n",
       "  'x': [717, 717, 897, 897],\n",
       "  'y': [832, 932, 832, 932]},\n",
       " {'data': '060',\n",
       "  'id': 4,\n",
       "  'x': [1729, 1729, 1821, 1821],\n",
       "  'y': [769, 838, 769, 838]},\n",
       " {'data': '8978',\n",
       "  'id': 5,\n",
       "  'x': [1847, 1847, 1977, 1977],\n",
       "  'y': [776, 837, 776, 837]},\n",
       " {'data': '1426',\n",
       "  'id': 6,\n",
       "  'x': [2008, 2008, 2131, 2131],\n",
       "  'y': [770, 836, 770, 836]},\n",
       " {'data': '050',\n",
       "  'id': 7,\n",
       "  'x': [1730, 1730, 1816, 1816],\n",
       "  'y': [864, 928, 864, 928]},\n",
       " {'data': '7156',\n",
       "  'id': 8,\n",
       "  'x': [1838, 1838, 1948, 1948],\n",
       "  'y': [868, 925, 868, 925]},\n",
       " {'data': '3106',\n",
       "  'id': 9,\n",
       "  'x': [1967, 1967, 2083, 2083],\n",
       "  'y': [870, 931, 870, 931]},\n",
       " {'data': '워라밸',\n",
       "  'id': 10,\n",
       "  'x': [749, 749, 919, 919],\n",
       "  'y': [1163, 1251, 1163, 1251]},\n",
       " {'data': '030',\n",
       "  'id': 11,\n",
       "  'x': [1199, 1199, 1275, 1275],\n",
       "  'y': [1211, 1271, 1211, 1271]},\n",
       " {'data': '9300',\n",
       "  'id': 12,\n",
       "  'x': [1296, 1296, 1391, 1391],\n",
       "  'y': [1210, 1271, 1210, 1271]},\n",
       " {'data': '6752',\n",
       "  'id': 13,\n",
       "  'x': [1418, 1418, 1520, 1520],\n",
       "  'y': [1211, 1274, 1211, 1274]},\n",
       " {'data': '090',\n",
       "  'id': 14,\n",
       "  'x': [1546, 1546, 1625, 1625],\n",
       "  'y': [1210, 1280, 1210, 1280]},\n",
       " {'data': '7345',\n",
       "  'id': 15,\n",
       "  'x': [1649, 1649, 1763, 1763],\n",
       "  'y': [1216, 1283, 1216, 1283]},\n",
       " {'data': '0115',\n",
       "  'id': 16,\n",
       "  'x': [1779, 1779, 1870, 1870],\n",
       "  'y': [1217, 1274, 1217, 1274]},\n",
       " {'data': '유앙페',\n",
       "  'id': 17,\n",
       "  'x': [417, 417, 596, 596],\n",
       "  'y': [1384, 1476, 1384, 1476]},\n",
       " {'data': '291962',\n",
       "  'id': 18,\n",
       "  'x': [1002, 1002, 1172, 1172],\n",
       "  'y': [1374, 1449, 1374, 1449]},\n",
       " {'data': '326056',\n",
       "  'id': 19,\n",
       "  'x': [1279, 1279, 1455, 1455],\n",
       "  'y': [1371, 1453, 1371, 1453]},\n",
       " {'data': '제주특별자치도',\n",
       "  'id': 20,\n",
       "  'x': [737, 737, 1044, 1044],\n",
       "  'y': [1531, 1614, 1531, 1614]},\n",
       " {'data': '단양군',\n",
       "  'id': 21,\n",
       "  'x': [1057, 1057, 1203, 1203],\n",
       "  'y': [1534, 1620, 1534, 1620]},\n",
       " {'data': '칠괴동',\n",
       "  'id': 22,\n",
       "  'x': [1222, 1222, 1361, 1361],\n",
       "  'y': [1534, 1616, 1534, 1616]},\n",
       " {'data': '종로3가',\n",
       "  'id': 23,\n",
       "  'x': [1379, 1379, 1540, 1540],\n",
       "  'y': [1534, 1619, 1534, 1619]},\n",
       " {'data': '인천광역시',\n",
       "  'id': 24,\n",
       "  'x': [746, 746, 966, 966],\n",
       "  'y': [1633, 1716, 1633, 1716]},\n",
       " {'data': '수원시',\n",
       "  'id': 25,\n",
       "  'x': [986, 986, 1129, 1129],\n",
       "  'y': [1642, 1715, 1642, 1715]},\n",
       " {'data': '사천시',\n",
       "  'id': 26,\n",
       "  'x': [1158, 1158, 1293, 1293],\n",
       "  'y': [1641, 1714, 1641, 1714]},\n",
       " {'data': '함양군',\n",
       "  'id': 27,\n",
       "  'x': [1317, 1317, 1474, 1474],\n",
       "  'y': [1639, 1718, 1639, 1718]},\n",
       " {'data': '99',\n",
       "  'id': 28,\n",
       "  'x': [1939, 1939, 2007, 2007],\n",
       "  'y': [1545, 1615, 1545, 1615]},\n",
       " {'data': '383',\n",
       "  'id': 29,\n",
       "  'x': [2084, 2084, 2165, 2165],\n",
       "  'y': [1550, 1619, 1550, 1619]},\n",
       " {'data': '671',\n",
       "  'id': 30,\n",
       "  'x': [344, 344, 444, 444],\n",
       "  'y': [2485, 2549, 2485, 2549]},\n",
       " {'data': '714',\n",
       "  'id': 31,\n",
       "  'x': [527, 527, 624, 624],\n",
       "  'y': [2486, 2553, 2486, 2553]},\n",
       " {'data': '법블레스유',\n",
       "  'id': 32,\n",
       "  'x': [961, 961, 1151, 1151],\n",
       "  'y': [2486, 2552, 2486, 2552]},\n",
       " {'data': '93',\n",
       "  'id': 33,\n",
       "  'x': [1233, 1233, 1303, 1303],\n",
       "  'y': [2489, 2554, 2489, 2554]},\n",
       " {'data': '3799',\n",
       "  'id': 34,\n",
       "  'x': [1332, 1332, 1464, 1464],\n",
       "  'y': [2486, 2558, 2486, 2558]},\n",
       " {'data': '635377',\n",
       "  'id': 35,\n",
       "  'x': [1613, 1613, 1797, 1797],\n",
       "  'y': [2483, 2558, 2483, 2558]},\n",
       " {'data': '937487',\n",
       "  'id': 36,\n",
       "  'x': [1841, 1841, 2047, 2047],\n",
       "  'y': [2491, 2563, 2491, 2563]},\n",
       " {'data': '2041',\n",
       "  'id': 37,\n",
       "  'x': [337, 337, 444, 444],\n",
       "  'y': [2645, 2704, 2645, 2704]},\n",
       " {'data': '31',\n",
       "  'id': 38,\n",
       "  'x': [495, 495, 565, 565],\n",
       "  'y': [2650, 2706, 2650, 2706]},\n",
       " {'data': '93',\n",
       "  'id': 39,\n",
       "  'x': [631, 631, 707, 707],\n",
       "  'y': [2645, 2704, 2645, 2704]},\n",
       " {'data': '4171',\n",
       "  'id': 40,\n",
       "  'x': [1284, 1284, 1401, 1401],\n",
       "  'y': [2649, 2709, 2649, 2709]},\n",
       " {'data': '5317',\n",
       "  'id': 41,\n",
       "  'x': [1808, 1808, 1937, 1937],\n",
       "  'y': [2650, 2714, 2650, 2714]},\n",
       " {'data': '9769',\n",
       "  'id': 42,\n",
       "  'x': [669, 669, 804, 804],\n",
       "  'y': [2965, 3024, 2965, 3024]},\n",
       " {'data': '2032',\n",
       "  'id': 43,\n",
       "  'x': [1175, 1175, 1292, 1292],\n",
       "  'y': [2965, 3028, 2965, 3028]},\n",
       " {'data': '53',\n",
       "  'id': 44,\n",
       "  'x': [1317, 1317, 1389, 1389],\n",
       "  'y': [2974, 3028, 2974, 3028]},\n",
       " {'data': '46',\n",
       "  'id': 45,\n",
       "  'x': [1416, 1416, 1483, 1483],\n",
       "  'y': [2971, 3030, 2971, 3030]},\n",
       " {'data': '유도순',\n",
       "  'id': 46,\n",
       "  'x': [455, 455, 594, 594],\n",
       "  'y': [3103, 3188, 3103, 3188]},\n",
       " {'data': '474733',\n",
       "  'id': 47,\n",
       "  'x': [616, 616, 773, 773],\n",
       "  'y': [3109, 3179, 3109, 3179]},\n",
       " {'data': '687987',\n",
       "  'id': 48,\n",
       "  'x': [774, 774, 909, 909],\n",
       "  'y': [3115, 3188, 3115, 3188]},\n",
       " {'data': '누나',\n",
       "  'id': 49,\n",
       "  'x': [1114, 1114, 1222, 1222],\n",
       "  'y': [3117, 3196, 3117, 3196]},\n",
       " {'data': '여',\n",
       "  'id': 50,\n",
       "  'x': [1385, 1385, 1482, 1482],\n",
       "  'y': [3113, 3188, 3113, 3188]},\n",
       " {'data': '탁정라',\n",
       "  'id': 51,\n",
       "  'x': [453, 453, 599, 599],\n",
       "  'y': [3208, 3285, 3208, 3285]},\n",
       " {'data': '335294',\n",
       "  'id': 52,\n",
       "  'x': [613, 613, 751, 751],\n",
       "  'y': [3219, 3289, 3219, 3289]},\n",
       " {'data': '682536',\n",
       "  'id': 53,\n",
       "  'x': [774, 774, 908, 908],\n",
       "  'y': [3221, 3278, 3221, 3278]},\n",
       " {'data': '손',\n",
       "  'id': 54,\n",
       "  'x': [1126, 1126, 1203, 1203],\n",
       "  'y': [3219, 3290, 3219, 3290]},\n",
       " {'data': '부',\n",
       "  'id': 55,\n",
       "  'x': [1396, 1396, 1466, 1466],\n",
       "  'y': [3211, 3293, 3211, 3293]},\n",
       " {'data': '주', 'id': 56, 'x': [755, 755, 790, 790], 'y': [656, 710, 656, 710]},\n",
       " {'data': '주',\n",
       "  'id': 57,\n",
       "  'x': [689, 689, 727, 727],\n",
       "  'y': [1164, 1227, 1164, 1227]},\n",
       " {'data': '주',\n",
       "  'id': 58,\n",
       "  'x': [913, 913, 944, 944],\n",
       "  'y': [2498, 2544, 2498, 2544]}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# JSON 파일 경로\n",
    "json_file_path = 'C:/upload/IMG_OCR_53_4PO_09451.json'\n",
    "\n",
    "# JSON 파일을 읽어와서 지정된 범위의 데이터를 추출\n",
    "with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    json_lines = json_file.readlines()[29:-1]  # 30번째 줄부터 마지막 줄의 이전 줄까지\n",
    "\n",
    "# 개행 문자를 제거하고 \"bbox\" 부분 제거하여 JSON 데이터로 파싱\n",
    "json_data = ''.join(line.strip().replace('\"bbox\": ', '') for line in json_lines)\n",
    "data = json.loads(json_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 원래 한글 문장\n",
    "original_text = \"안녕하세요, 한글 tokenizer 인코딩 예제입니다.\"\n",
    "\n",
    "# tokenizer 객체 생성\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "\n",
    "# tokenizer를 학습하여 문장을 숫자로 인코딩\n",
    "tokenizer.fit_on_texts([original_text])\n",
    "\n",
    "# 인코딩된 데이터 생성\n",
    "encoded_data = tokenizer.texts_to_sequences([original_text])[0]\n",
    "\n",
    "# 인코딩된 데이터 출력\n",
    "print(encoded_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbaf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스를 문자로 변환하여 디코딩된 문장 생성\n",
    "decoded_text = tokenizer.sequences_to_texts([encoded_data])[0]\n",
    "\n",
    "# 디코딩된 문장 출력\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf89393e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf5c4a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# JSON 파일 경로\n",
    "json_file_path = 'C:/upload/IMG_OCR_53_4PO_09451.json'\n",
    "\n",
    "# JSON 파일을 읽어와서 지정된 범위의 데이터를 추출\n",
    "with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    json_lines = json_file.readlines()[29:-1]  # 30번째 줄부터 마지막 줄의 이전 줄까지\n",
    "\n",
    "# 개행 문자를 제거하고 \"bbox\" 부분 제거하여 JSON 데이터로 파싱\n",
    "source_json_data = ''.join(line.strip().replace('\"bbox\": ', '') for line in json_lines)\n",
    "json_data = json.loads(source_json_data)\n",
    "\n",
    "# Tokenizer 생성 및 데이터 학습\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "data = [item[\"data\"] for item in json_data]\n",
    "tokenizer.fit_on_texts(data)\n",
    "\n",
    "# 데이터 인코딩\n",
    "new_data = []\n",
    "encoded_data_list = []\n",
    "for item in json_data:\n",
    "    encoded_data = tokenizer.texts_to_sequences([item[\"data\"]])[0]\n",
    "    new_item = {\n",
    "        \"data\": encoded_data,\n",
    "        \"id\": item[\"id\"],\n",
    "        \"x\": item[\"x\"],\n",
    "        \"y\": item[\"y\"]\n",
    "    }\n",
    "    new_data.append(new_item)\n",
    "\n",
    "# 결과 출력 (예시로 처음 5개 데이터만 출력)\n",
    "for idx, item in enumerate(new_data[:5]):\n",
    "    print(f\"Data {idx + 1}:\")\n",
    "    print(\"  'data':\", item[\"data\"])\n",
    "    print(\"  'id':\", item[\"id\"])\n",
    "    print(\"  'x':\", item[\"x\"])\n",
    "    print(\"  'y':\", item[\"y\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코딩 함수 정의\n",
    "def decode_sequence(encoded_sequence):\n",
    "    return tokenizer.sequences_to_texts([encoded_sequence])[0]\n",
    "\n",
    "# 결과 출력 및 디코딩 (예시로 처음 5개 데이터만 출력)\n",
    "for idx, item in enumerate(new_data[:5]):\n",
    "    print(f\"Data {idx + 1}:\")\n",
    "    print(\"  'data':\", decode_sequence(item[\"data\"]))\n",
    "    print(\"  'id':\", item[\"id\"])\n",
    "    print(\"  'x':\", item[\"x\"])\n",
    "    print(\"  'y':\", item[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51351c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3400a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
